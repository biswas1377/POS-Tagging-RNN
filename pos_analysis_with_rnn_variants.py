# -*- coding: utf-8 -*-
"""pos_analysis_with_RNN_variants

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aETdYxTLDzPa0WxmCXBesHJJJE738Mr1
"""

from google.colab import drive
drive.mount('/content/drive')

# Load libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import ast
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Dense, TimeDistributed, Dropout, Masking
from tensorflow.keras.optimizers import Adam

# Load datasets
train_df = pd.read_csv('/content/drive/MyDrive/Untitled Folder/Dataset_B_POS_train.csv')
test_df = pd.read_csv('/content/drive/MyDrive/Untitled Folder/Dataset_B_POS_test.csv')


train_df.head()

"""## Exploratory Data Analysis
- Check dataset shape, columns, and sample rows
- Analyze sentence lengths and POS tag distributions
"""

# Inspect data
print('Train shape:', train_df.shape)
print('Test shape:', test_df.shape)
print(train_df.columns)
print(train_df.iloc[0])

# Parse sentences and tags with proper handling of punctuation
import re
import ast
import matplotlib.pyplot as plt

def parse_row(row):
    # Get original sentence and tags
    original_words = row['Sentence'].split()
    tags = ast.literal_eval(row['POS'])

    # For analysis purposes, we'll separate words and punctuation
    # But we'll keep the original mapping to tags
    return original_words, tags

# Enhanced tokenize function to separate punctuation for analysis
def tokenize_for_analysis(sentence_list):
    """Separates punctuation from words, but only for analysis purposes"""
    word_only_counts = []
    for sentence in sentence_list:
        # Count only alphabetic words (no punctuation-only tokens)
        word_count = sum(1 for word in sentence if re.search('[a-zA-Z]', word))
        word_only_counts.append(word_count)
    return word_only_counts

train_sentences = []
train_tags = []
for _, row in train_df.iterrows():
    words, tags = parse_row(row)
    train_sentences.append(words)
    train_tags.append(tags)

test_sentences = []
test_tags = []
for _, row in test_df.iterrows():
    words, tags = parse_row(row)
    test_sentences.append(words)
    test_tags.append(tags)

# Get counts with and without punctuation
train_sent_lens = [len(s) for s in train_sentences]
train_word_only_lens = tokenize_for_analysis(train_sentences)

test_sent_lens = [len(s) for s in test_sentences]
test_word_only_lens = tokenize_for_analysis(test_sentences)

# Sentence length distribution for train data - FIXED
plt.figure(figsize=(12, 6))

# Create a side-by-side bar plot for with/without punctuation
plt.subplot(1, 2, 1)
counts, bins, _ = plt.hist(train_sent_lens, bins=20, edgecolor='black', color='skyblue', density=False, alpha=0.7, label='With Punctuation')
plt.hist(train_word_only_lens, bins=bins, edgecolor='black', color='orange', density=False, alpha=0.7, label='Words Only')

# Add count labels to the with-punctuation bars
for i in range(len(counts)):
    if counts[i] > 0:
        plt.text(bins[i] + (bins[i+1]-bins[i])/2, counts[i],
                 str(int(counts[i])), ha='center', va='bottom', fontsize=8)

plt.title('Train Data: Sentence Length Distribution')
plt.xlabel('Length')
plt.ylabel('Count')
plt.grid(axis='y', alpha=0.75)
plt.legend()

# Sentence length distribution for test data
plt.subplot(1, 2, 2)
counts, bins, _ = plt.hist(test_sent_lens, bins=20, edgecolor='black', color='skyblue', density=False, alpha=0.7, label='With Punctuation')
plt.hist(test_word_only_lens, bins=bins, edgecolor='black', color='orange', density=False, alpha=0.7, label='Words Only')

# Add count labels to the with-punctuation bars
for i in range(len(counts)):
    if counts[i] > 0:
        plt.text(bins[i] + (bins[i+1]-bins[i])/2, counts[i],
                 str(int(counts[i])), ha='center', va='bottom', fontsize=8)

plt.title('Test Data: Sentence Length Distribution')
plt.xlabel('Length')
plt.ylabel('Count')
plt.grid(axis='y', alpha=0.75)
plt.legend()

plt.tight_layout()
plt.show()

# Add summary statistics for train data
print("--- TRAIN DATA STATISTICS ---")
print(f"Total sentences: {len(train_sent_lens)}")
print(f"Average sentence length (with punctuation): {sum(train_sent_lens)/len(train_sent_lens):.2f}")
print(f"Average sentence length (words only): {sum(train_word_only_lens)/len(train_word_only_lens):.2f}")
print(f"Min sentence length (with punctuation): {min(train_sent_lens)}")
print(f"Min sentence length (words only): {min(train_word_only_lens)}")
print(f"Max sentence length (with punctuation): {max(train_sent_lens)}")
print(f"Max sentence length (words only): {max(train_word_only_lens)}")

# Check if words and tags match in train data
train_mismatches = [(i, len(s), len(t)) for i, (s, t) in enumerate(zip(train_sentences, train_tags)) if len(s) != len(t)]
if train_mismatches:
    print(f"WARNING: Found {len(train_mismatches)} sentences where token count doesn't match tag count")
    for idx, word_count, tag_count in train_mismatches[:5]:  # Show first 5 mismatches
        print(f"  Sentence {idx}: {word_count} tokens, {tag_count} tags")
    if len(train_mismatches) > 5:
        print(f"  ... and {len(train_mismatches) - 5} more")

# Add summary statistics for test data
print("\n--- TEST DATA STATISTICS ---")
print(f"Total sentences: {len(test_sent_lens)}")
print(f"Average sentence length (with punctuation): {sum(test_sent_lens)/len(test_sent_lens):.2f}")
print(f"Average sentence length (words only): {sum(test_word_only_lens)/len(test_word_only_lens):.2f}")
print(f"Min sentence length (with punctuation): {min(test_sent_lens)}")
print(f"Min sentence length (words only): {min(test_word_only_lens)}")
print(f"Max sentence length (with punctuation): {max(test_sent_lens)}")
print(f"Max sentence length (words only): {max(test_word_only_lens)}")

# Check if words and tags match in test data
test_mismatches = [(i, len(s), len(t)) for i, (s, t) in enumerate(zip(test_sentences, test_tags)) if len(s) != len(t)]
if test_mismatches:
    print(f"WARNING: Found {len(test_mismatches)} sentences where token count doesn't match tag count")
    for idx, word_count, tag_count in test_mismatches[:5]:  # Show first 5 mismatches
        print(f"  Sentence {idx}: {word_count} tokens, {tag_count} tags")
    if len(test_mismatches) > 5:
        print(f"  ... and {len(test_mismatches) - 5} more")

# Find the longest sentence in train data
longest_train_idx = train_sent_lens.index(max(train_sent_lens))
print("\n--- LONGEST SENTENCE IN TRAIN DATA ---")
print(f"Length: {len(train_sentences[longest_train_idx])} tokens ({train_word_only_lens[longest_train_idx]} words)")
print(f"Sentence: {' '.join(train_sentences[longest_train_idx])}")

# Find the longest sentence in test data
longest_test_idx = test_sent_lens.index(max(test_sent_lens))
print("\n--- LONGEST SENTENCE IN TEST DATA ---")
print(f"Length: {len(test_sentences[longest_test_idx])} tokens ({test_word_only_lens[longest_test_idx]} words)")
print(f"Sentence: {' '.join(test_sentences[longest_test_idx])}")

# Find the most common POS tags in train data
train_tag_counts = {}
for tags in train_tags:
    for tag in tags:
        train_tag_counts[tag] = train_tag_counts.get(tag, 0) + 1

print("\n--- MOST COMMON POS TAGS IN TRAIN DATA ---")
print(f"Total unique tags in train data: {len(train_tag_counts)}")
print(f"Total tag occurrences in train data: {sum(train_tag_counts.values())}")
sorted_train_tags = sorted(train_tag_counts.items(), key=lambda x: x[1], reverse=True)
for tag, count in sorted_train_tags[:2]:
    print(f"Tag: {tag}, Count: {count}")

# Find the most common POS tags in test data
test_tag_counts = {}
for tags in test_tags:
    for tag in tags:
        test_tag_counts[tag] = test_tag_counts.get(tag, 0) + 1

print("\n--- MOST COMMON POS TAGS IN TEST DATA ---")
print(f"Total unique tags in test data: {len(test_tag_counts)}")
print(f"Total tag occurrences in test data: {sum(test_tag_counts.values())}")
sorted_test_tags = sorted(test_tag_counts.items(), key=lambda x: x[1], reverse=True)
for tag, count in sorted_test_tags[:2]:
    print(f"Tag: {tag}, Count: {count}")

# Visualize tag distribution for train data
plt.figure(figsize=(15, 8))
tags = [tag for tag, _ in sorted_train_tags]  # Get all tags
counts = [count for _, count in sorted_train_tags]  # Get all counts

# Create bar chart
bars = plt.bar(range(len(tags)), counts, color='skyblue', edgecolor='black', width=0.7)
plt.title('All POS Tags in Training Data')
plt.xlabel('POS Tag')
plt.ylabel('Count')
plt.xticks(range(len(tags)), tags, rotation=90)

# Only add labels to bars that are tall enough to be visible
threshold = max(counts) * 0.05  # Only label bars with counts above 5% of the max
for i, (bar, count) in enumerate(zip(bars, counts)):
    if count >= threshold:
        plt.text(i, count + (max(counts) * 0.01), str(count), ha='center', fontsize=8)

plt.tight_layout()
plt.grid(axis='y', alpha=0.3)
plt.show()

# Visualize tag distribution for test data
plt.figure(figsize=(15, 8))
tags = [tag for tag, _ in sorted_test_tags]  # Get all tags
counts = [count for _, count in sorted_test_tags]  # Get all counts

# Create bar chart
bars = plt.bar(range(len(tags)), counts, color='lightgreen', edgecolor='black', width=0.7)
plt.title('All POS Tags in Test Data')
plt.xlabel('POS Tag')
plt.ylabel('Count')
plt.xticks(range(len(tags)), tags, rotation=90)

# Only add labels to bars that are tall enough to be visible
threshold = max(counts) * 0.05  # Only label bars with counts above 5% of the max
for i, (bar, count) in enumerate(zip(bars, counts)):
    if count >= threshold:
        plt.text(i, count + (max(counts) * 0.01), str(count), ha='center', fontsize=8)

plt.tight_layout()
plt.grid(axis='y', alpha=0.3)
plt.show()

# Compare word-only vs with-punctuation counts
plt.figure(figsize=(10, 6))
plt.scatter(train_sent_lens, train_word_only_lens, alpha=0.5, label='Train Sentences')
plt.scatter(test_sent_lens, test_word_only_lens, alpha=0.5, color='red', label='Test Sentences')
plt.plot([0, max(max(train_sent_lens), max(test_sent_lens))],
         [0, max(max(train_sent_lens), max(test_sent_lens))], 'k--', alpha=0.3)
plt.xlabel('Token Count (with punctuation)')
plt.ylabel('Word Count (no punctuation)')
plt.title('Comparison of Token vs Word Counts')
plt.grid(alpha=0.3)
plt.legend()
plt.tight_layout()
plt.show()

# Analyze punctuation percentage
train_punc_percentage = [(total - words)/total*100 if total > 0 else 0
                          for total, words in zip(train_sent_lens, train_word_only_lens)]
test_punc_percentage = [(total - words)/total*100 if total > 0 else 0
                         for total, words in zip(test_sent_lens, test_word_only_lens)]

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.hist(train_punc_percentage, bins=20, color='skyblue', edgecolor='black')
plt.title('Train Data: Punctuation Percentage')
plt.xlabel('Punctuation %')
plt.ylabel('Sentence Count')
plt.grid(alpha=0.3)

plt.subplot(1, 2, 2)
plt.hist(test_punc_percentage, bins=20, color='lightgreen', edgecolor='black')
plt.title('Test Data: Punctuation Percentage')
plt.xlabel('Punctuation %')
plt.ylabel('Sentence Count')
plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()

print("\n--- PUNCTUATION STATISTICS ---")
print(f"Average punctuation percentage in train data: {sum(train_punc_percentage)/len(train_punc_percentage):.2f}%")
print(f"Average punctuation percentage in test data: {sum(test_punc_percentage)/len(test_punc_percentage):.2f}%")

"""## Text Preprocessing
- Build word and tag vocabularies
- Encode sentences and tags
- Pad sequences
"""

# Build vocabularies
words = set(w for s in train_sentences for w in s)
tags = set(t for ts in train_tags for t in ts)
word2idx = {w: i+2 for i, w in enumerate(sorted(words))}
word2idx['PAD'] = 0
word2idx['UNK'] = 1
idx2word = {i: w for w, i in word2idx.items()}

tag2idx = {t: i for i, t in enumerate(sorted(tags))}
idx2tag = {i: t for t, i in tag2idx.items()}

# Encode and pad
max_len = max(max(len(s) for s in train_sentences), max(len(s) for s in test_sentences))
def encode_sentences(sentences, word2idx, max_len):
    return pad_sequences([[word2idx.get(w, word2idx['UNK']) for w in s] for s in sentences],
                        maxlen=max_len, padding='post', value=word2idx['PAD'])
def encode_tags(tags_list, tag2idx, max_len):
    return pad_sequences([[tag2idx[t] for t in ts] for ts in tags_list],
                        maxlen=max_len, padding='post', value=tag2idx[list(tag2idx.keys())[0]])

X_train = encode_sentences(train_sentences, word2idx, max_len)
y_train = encode_tags(train_tags, tag2idx, max_len)
X_test = encode_sentences(test_sentences, word2idx, max_len)
y_test = encode_tags(test_tags, tag2idx, max_len)

# One-hot encode y
y_train_cat = to_categorical(y_train, num_classes=len(tag2idx))
y_test_cat = to_categorical(y_test, num_classes=len(tag2idx))

"""## Model Definitions
- RNN, LSTM, GRU, BiLSTM architectures
- Use Embedding + RNN variant + TimeDistributed(Dense)
"""

def build_model(cell_type='rnn', units=64, lr=0.001, dropout=0.2):
    model = Sequential()
    # Remove mask_zero=True to avoid creating the boolean mask
    model.add(Embedding(input_dim=len(word2idx), output_dim=64, input_length=max_len))
    if cell_type == 'rnn':
        model.add(SimpleRNN(units, return_sequences=True, dropout=dropout))
    elif cell_type == 'lstm':
        model.add(LSTM(units, return_sequences=True, dropout=dropout))
    elif cell_type == 'gru':
        model.add(GRU(units, return_sequences=True, dropout=dropout))
    elif cell_type == 'bilstm':
        model.add(Bidirectional(LSTM(units, return_sequences=True, dropout=dropout)))
    model.add(TimeDistributed(Dense(len(tag2idx), activation='softmax')))
    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

"""## Hyperparameter Tuning & Training
- Try different batch sizes, learning rates, units, epochs
- Save training history for comparison
"""

import matplotlib.pyplot as plt

results = {}
histories = {}
models = {}

params = {
    'batch_size': 64,
    'epochs': 20,
    'units': 64,
    'lr': 0.01,
    'dropout': 0.2
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()}...')

    model = build_model(cell_type=cell, units=params['units'], lr=params['lr'], dropout=params['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params['batch_size'],
        epochs=params['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models[cell] = model
    histories[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

"""## Evaluation
- Predict on test set
- Compute accuracy, F1-score, confusion matrix, classification report
- Visualize confusion matrix and metrics
"""

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating RNN...')

# Get the RNN model
model = models['rnn']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_rnn = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_rnn,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_rnn).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: RNN')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_rnn = acc
f1_rnn = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating LSTM...')

# Get the LSTM model
model = models['lstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_lstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_lstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_lstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: LSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_lstm = acc
f1_lstm = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating GRU...')

# Get the GRU model
model = models['gru']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_gru = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_gru,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_gru).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: GRU')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_gru = acc
f1_gru = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating BiLSTM...')

# Get the BiLSTM model
model = models['bilstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_bilstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_bilstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_bilstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: BiLSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_bilstm = acc
f1_bilstm = f1

"""## Model Comparison
- Tabular and visual comparison of all models
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

# Create a dictionary to store the metrics
metrics = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_rnn, acc_lstm, acc_gru, acc_bilstm],
    'F1-score': [f1_rnn, f1_lstm, f1_gru, f1_bilstm]
}

# Create a Pandas DataFrame from the metrics dictionary
metrics_df = pd.DataFrame(metrics)

# Set plot style
sns.set_theme(style="whitegrid")

# Create figure and axes
fig, ax = plt.subplots(figsize=(12, 8))  # Bigger figure

# Plot the bar chart
metrics_df.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)

# Customize title and labels
ax.set_title('Model Performance Comparison', fontsize=20, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)

# Add grid
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)

# Add value labels on bars
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')

# Adjust layout
plt.tight_layout()
plt.show()

"""# Decreased batch size,epoch,neurons and learning rate. Increased dropout"""

import matplotlib.pyplot as plt

results = {}
histories = {}
models = {}

params = {
    'batch_size': 10,
    'epochs': 5,
    'units': 32,
    'lr': 0.001,
    'dropout': 0.4
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()}...')

    model = build_model(cell_type=cell, units=params['units'], lr=params['lr'], dropout=params['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params['batch_size'],
        epochs=params['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models[cell] = model
    histories[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating RNN...')

# Get the RNN model
model = models['rnn']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_rnn = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_rnn,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_rnn).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: RNN')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_rnn = acc
f1_rnn = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating LSTM...')

# Get the LSTM model
model = models['lstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_lstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_lstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_lstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: LSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_lstm = acc
f1_lstm = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating GRU...')

# Get the GRU model
model = models['gru']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_gru = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_gru,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_gru).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: GRU')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_gru = acc
f1_gru = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating BiLSTM...')

# Get the BiLSTM model
model = models['bilstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_bilstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_bilstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_bilstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: BiLSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_bilstm = acc
f1_bilstm = f1

# Create a dictionary to store the metrics
metrics = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_rnn, acc_lstm, acc_gru, acc_bilstm],
    'F1-score': [f1_rnn, f1_lstm, f1_gru, f1_bilstm]
}

# Create a Pandas DataFrame from the metrics dictionary
metrics_df = pd.DataFrame(metrics)

# Set plot style
sns.set_theme(style="whitegrid")

# Create figure and axes
fig, ax = plt.subplots(figsize=(12, 8))  # Bigger figure

# Plot the bar chart
metrics_df.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)

# Customize title and labels
ax.set_title('Model Performance Comparison', fontsize=20, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)

# Add grid
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)

# Add value labels on bars
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')

# Adjust layout
plt.tight_layout()
plt.show()

"""# more epochs-(30 epocs), dropout back to 0.2"""

import matplotlib.pyplot as plt

results = {}
histories = {}
models = {}

params = {
    'batch_size': 10,
    'epochs': 30,
    'units': 32,
    'lr': 0.001,
    'dropout': 0.2
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()}...')

    model = build_model(cell_type=cell, units=params['units'], lr=params['lr'], dropout=params['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params['batch_size'],
        epochs=params['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models[cell] = model
    histories[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating RNN...')

# Get the RNN model
model = models['rnn']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_rnn = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_rnn,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_rnn).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: RNN')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_rnn = acc
f1_rnn = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating LSTM...')

# Get the LSTM model
model = models['lstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_lstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_lstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_lstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: LSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_lstm = acc
f1_lstm = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating GRU...')

# Get the GRU model
model = models['gru']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_gru = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_gru,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_gru).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: GRU')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_gru = acc
f1_gru = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating BiLSTM...')

# Get the BiLSTM model
model = models['bilstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_bilstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_bilstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_bilstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: BiLSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_bilstm = acc
f1_bilstm = f1

# Create a dictionary to store the metrics
metrics = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_rnn, acc_lstm, acc_gru, acc_bilstm],
    'F1-score': [f1_rnn, f1_lstm, f1_gru, f1_bilstm]
}

# Create a Pandas DataFrame from the metrics dictionary
metrics_df = pd.DataFrame(metrics)

# Set plot style
sns.set_theme(style="whitegrid")

# Create figure and axes
fig, ax = plt.subplots(figsize=(12, 8))  # Bigger figure

# Plot the bar chart
metrics_df.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)

# Customize title and labels
ax.set_title('Model Performance Comparison', fontsize=20, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)

# Add grid
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)

# Add value labels on bars
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')

# Adjust layout
plt.tight_layout()
plt.show()

"""# 10 epochs

"""

#hypermarameter training and testing

results = {}
histories = {}
models = {}
params = {
    'batch_size': 32,
    'epochs': 10,
    'units': 64,
    'lr': 0.001,
    'dropout': 0.2
}
for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()}...')

    model = build_model(cell_type=cell, units=params['units'], lr=params['lr'], dropout=params['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params['batch_size'],
        epochs=params['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models[cell] = model
    histories[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating RNN...')

# Get the RNN model
model = models['rnn']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_rnn = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_rnn,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_rnn).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: RNN')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_rnn = acc
f1_rnn = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating LSTM...')

# Get the LSTM model
model = models['lstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_lstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_lstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_lstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: LSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_lstm = acc
f1_lstm = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating GRU...')

# Get the GRU model
model = models['gru']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_gru = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_gru,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_gru).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: GRU')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_gru = acc
f1_gru = f1

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

print(f'\nEvaluating BiLSTM...')

# Get the BiLSTM model
model = models['bilstm']

# Predict
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
y_true_labels = y_test.flatten()

# Define padding index (assuming first key in tag2idx is 'PAD' or similar)
padding_idx = tag2idx[list(tag2idx.keys())[0]]

# Remove padding
mask = y_true_labels != padding_idx
y_true_labels = y_true_labels[mask]
y_pred_labels = y_pred_labels[mask]

# Accuracy and F1
acc = accuracy_score(y_true_labels, y_pred_labels)
f1 = f1_score(y_true_labels, y_pred_labels, average='macro')

# Build label and name list from idx2tag (excluding padding)
label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
target_names_bilstm = [idx2tag[i] for i in label_indices]

# Classification report and confusion matrix
report = classification_report(
    y_true_labels, y_pred_labels,
    labels=label_indices,
    target_names=target_names_bilstm,
    zero_division=0
)
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)

# Output
print('Accuracy:', acc)
print('F1-score:', f1)
print('Classification Report:\n', report)

# Create a larger plot with higher dpi for better clarity
fig, ax = plt.subplots(figsize=(35, 28), dpi=150)  # Increasing dpi for better resolution
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names_bilstm).plot(ax=ax, xticks_rotation=45)
plt.title('Confusion Matrix: BiLSTM')
plt.tight_layout()
plt.show()

# Assign accuracy and F1-score to variables for later comparison
acc_bilstm = acc
f1_bilstm = f1

#model comparison using chart

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

# Create a dictionary to store the metrics
metrics = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_rnn, acc_lstm, acc_gru, acc_bilstm],
    'F1-score': [f1_rnn, f1_lstm, f1_gru, f1_bilstm]
}

# Create a Pandas DataFrame from the metrics dictionary
metrics_df = pd.DataFrame(metrics)

# Set plot style
sns.set_theme(style="whitegrid")

# Create figure and axes
fig, ax = plt.subplots(figsize=(12, 8))  # Bigger figure

# Plot the bar chart
metrics_df.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)

# Customize title and labels
ax.set_title('Model Performance Comparison', fontsize=20, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)

# Add grid
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)

# Add value labels on bars
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')

# Adjust layout
plt.tight_layout()
plt.show()

"""# Word2Vec Embedding Integration

We will use pre-trained Word2Vec embeddings (Google News vectors) for the embedding layer. If not available, we will train our own Word2Vec on the training data. Then, we will re-train all four models (RNN, LSTM, GRU, BiLSTM) using these embeddings and compare their results.
"""

!pip install gensim
#from gensim.models import Word2Vec
!pip uninstall numpy gensim -y
!pip install numpy gensim --no-cache-dir
!pip install --upgrade pip setuptools wheel
!pip install -U gensim numpy scipy

# Install gensim if not already installed
import gensim

from gensim.models import Word2Vec
import numpy as np

# Ensure train_sentences and train_tags are defined (in case of kernel reset or out-of-order execution)
import ast

def parse_row(row):
    words = row['Sentence'].split()
    tags = ast.literal_eval(row['POS'])
    return words, tags

train_sentences = []
train_tags = []
for _, row in train_df.iterrows():
    words, tags = parse_row(row)
    train_sentences.append(words)
    train_tags.append(tags)

# If needed, also re-define test_sentences and test_tags
test_sentences = []
test_tags = []
for _, row in test_df.iterrows():
    words, tags = parse_row(row)
    test_sentences.append(words)
    test_tags.append(tags)

# Train Word2Vec on training sentences (since Google News vectors are too large for Colab and not available by default)
embedding_dim = 100
w2v_model = Word2Vec(sentences=train_sentences, vector_size=embedding_dim, window=5, min_count=1, workers=4, seed=42)

# Build embedding matrix
embedding_matrix = np.zeros((len(word2idx), embedding_dim))
for word, idx in word2idx.items():
    if word in w2v_model.wv:
        embedding_matrix[idx] = w2v_model.wv[word]
    else:
        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))

"""## Model Definition with Word2Vec Embedding

We will define a new model builder that uses the pre-trained Word2Vec embedding matrix and freezes the embedding layer.
"""

from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Dense, TimeDistributed, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

def build_model_w2v(cell_type='rnn', units=64, lr=0.001, dropout=0.2):
    model = Sequential()
    model.add(Embedding(input_dim=len(word2idx), output_dim=embedding_dim,
                        input_length=max_len,
                        weights=[embedding_matrix],
                        trainable=False))
    if cell_type == 'rnn':
        model.add(SimpleRNN(units, return_sequences=True, dropout=dropout))
    elif cell_type == 'lstm':
        model.add(LSTM(units, return_sequences=True, dropout=dropout))
    elif cell_type == 'gru':
        model.add(GRU(units, return_sequences=True, dropout=dropout))
    elif cell_type == 'bilstm':
        model.add(Bidirectional(LSTM(units, return_sequences=True, dropout=dropout)))
    model.add(TimeDistributed(Dense(len(tag2idx), activation='softmax')))
    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

"""## Training All Models with Word2Vec Embedding -- 10 epocs

We will train RNN, LSTM, GRU, and BiLSTM models using the Word2Vec embedding for 10 epochs.
"""

results_w2v = {}
histories_w2v = {}
models_w2v = {}

params_w2v = {
    'batch_size': 32,
    'epochs': 10,
    'units': 64,
    'lr': 0.001,
    'dropout': 0.2
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()} with Word2Vec...')

    model = build_model_w2v(cell_type=cell, units=params_w2v['units'], lr=params_w2v['lr'], dropout=params_w2v['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params_w2v['batch_size'],
        epochs=params_w2v['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models_w2v[cell] = model
    histories_w2v[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} (Word2Vec) Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params_w2v['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

"""## Evaluation of Word2Vec Models

Evaluate each model trained with Word2Vec embedding on the test set and collect accuracy and F1-score.
"""

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix

acc_w2v = {}
f1_w2v = {}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nEvaluating {cell.upper()} (Word2Vec)...')
    model = models_w2v[cell]
    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
    y_true_labels = y_test.flatten()
    padding_idx = tag2idx[list(tag2idx.keys())[0]]
    mask = y_true_labels != padding_idx
    y_true_labels = y_true_labels[mask]
    y_pred_labels = y_pred_labels[mask]
    acc = accuracy_score(y_true_labels, y_pred_labels)
    f1 = f1_score(y_true_labels, y_pred_labels, average='weighted')
    acc_w2v[cell] = acc
    f1_w2v[cell] = f1
    label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
    target_names = [idx2tag[i] for i in label_indices]
    report = classification_report(
        y_true_labels, y_pred_labels,
        labels=label_indices,
        target_names=target_names,
        zero_division=0
    )
    cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)
    print('Accuracy:', acc)
    print('F1-score:', f1)
    print('Classification Report:\n', report)
    fig, ax = plt.subplots(figsize=(20, 16), dpi=100)
    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names).plot(ax=ax, xticks_rotation=45)
    plt.title(f'Confusion Matrix: {cell.upper()} (Word2Vec)')
    plt.tight_layout()
    plt.show()

"""## Word2Vec Model Performance Comparison"""

import pandas as pd
import seaborn as sns

metrics_w2v = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_w2v['rnn'], acc_w2v['lstm'], acc_w2v['gru'], acc_w2v['bilstm']],
    'F1-score': [f1_w2v['rnn'], f1_w2v['lstm'], f1_w2v['gru'], f1_w2v['bilstm']]
}
metrics_df_w2v = pd.DataFrame(metrics_w2v)
sns.set_theme(style="whitegrid")
fig, ax = plt.subplots(figsize=(12, 8))
metrics_df_w2v.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)
ax.set_title('Model Performance Comparison (Word2Vec) used weighted avg of F1 scores', fontsize=20, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')
plt.tight_layout()
plt.show()

"""# Using Macro avg to evaluate"""

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix

acc_w2v = {}
f1_w2v = {}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nEvaluating {cell.upper()} (Word2Vec)...')
    model = models_w2v[cell]
    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
    y_true_labels = y_test.flatten()
    padding_idx = tag2idx[list(tag2idx.keys())[0]]
    mask = y_true_labels != padding_idx
    y_true_labels = y_true_labels[mask]
    y_pred_labels = y_pred_labels[mask]
    acc = accuracy_score(y_true_labels, y_pred_labels)
    f1 = f1_score(y_true_labels, y_pred_labels, average='macro')
    acc_w2v[cell] = acc
    f1_w2v[cell] = f1
    label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
    target_names = [idx2tag[i] for i in label_indices]
    report = classification_report(
        y_true_labels, y_pred_labels,
        labels=label_indices,
        target_names=target_names,
        zero_division=0
    )
    cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)
    print('Accuracy:', acc)
    print('F1-score:', f1)
    print('Classification Report:\n', report)
    fig, ax = plt.subplots(figsize=(20, 16), dpi=100)
    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names).plot(ax=ax, xticks_rotation=45)
    plt.title(f'Confusion Matrix: {cell.upper()} (Word2Vec)')
    plt.tight_layout()
    plt.show()

import pandas as pd
import seaborn as sns

metrics_w2v = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_w2v['rnn'], acc_w2v['lstm'], acc_w2v['gru'], acc_w2v['bilstm']],
    'F1-score': [f1_w2v['rnn'], f1_w2v['lstm'], f1_w2v['gru'], f1_w2v['bilstm']]
}
metrics_df_w2v = pd.DataFrame(metrics_w2v)
sns.set_theme(style="whitegrid")
fig, ax = plt.subplots(figsize=(12, 8))
metrics_df_w2v.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)
ax.set_title('Model Performance Comparison (Word2Vec) (Used Macro Avg. of F1 scores) [epocs-10]', fontsize=14, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')
plt.tight_layout()
plt.show()

"""# Hyperparamaeter changed, now with 20 epocs"""

results_w2v = {}
histories_w2v = {}
models_w2v = {}

params_w2v = {
    'batch_size': 32,
    'epochs': 20,
    'units': 64,
    'lr': 0.001,
    'dropout': 0.2
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()} with Word2Vec...')

    model = build_model_w2v(cell_type=cell, units=params_w2v['units'], lr=params_w2v['lr'], dropout=params_w2v['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params_w2v['batch_size'],
        epochs=params_w2v['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models_w2v[cell] = model
    histories_w2v[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} (Word2Vec) Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params_w2v['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix

acc_w2v = {}
f1_w2v = {}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nEvaluating {cell.upper()} (Word2Vec)...')
    model = models_w2v[cell]
    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
    y_true_labels = y_test.flatten()
    padding_idx = tag2idx[list(tag2idx.keys())[0]]
    mask = y_true_labels != padding_idx
    y_true_labels = y_true_labels[mask]
    y_pred_labels = y_pred_labels[mask]
    acc = accuracy_score(y_true_labels, y_pred_labels)
    f1 = f1_score(y_true_labels, y_pred_labels, average='macro')
    acc_w2v[cell] = acc
    f1_w2v[cell] = f1
    label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
    target_names = [idx2tag[i] for i in label_indices]
    report = classification_report(
        y_true_labels, y_pred_labels,
        labels=label_indices,
        target_names=target_names,
        zero_division=0
    )
    cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)
    print('Accuracy:', acc)
    print('F1-score:', f1)
    print('Classification Report:\n', report)
    fig, ax = plt.subplots(figsize=(20, 16), dpi=100)
    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names).plot(ax=ax, xticks_rotation=45)
    plt.title(f'Confusion Matrix: {cell.upper()} (Word2Vec)')
    plt.tight_layout()
    plt.show()

import pandas as pd
import seaborn as sns

metrics_w2v = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_w2v['rnn'], acc_w2v['lstm'], acc_w2v['gru'], acc_w2v['bilstm']],
    'F1-score': [f1_w2v['rnn'], f1_w2v['lstm'], f1_w2v['gru'], f1_w2v['bilstm']]
}
metrics_df_w2v = pd.DataFrame(metrics_w2v)
sns.set_theme(style="whitegrid")
fig, ax = plt.subplots(figsize=(12, 8))
metrics_df_w2v.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)
ax.set_title('Model Performance Comparison (Word2Vec) (Used Macro Avg. of F1 scores) [Epocs-20]', fontsize=14, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')
plt.tight_layout()
plt.show()

"""# now, with 50 epocs"""

results_w2v = {}
histories_w2v = {}
models_w2v = {}

params_w2v = {
    'batch_size': 32,
    'epochs': 50,
    'units': 64,
    'lr': 0.001,
    'dropout': 0.2
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()} with Word2Vec...')

    model = build_model_w2v(cell_type=cell, units=params_w2v['units'], lr=params_w2v['lr'], dropout=params_w2v['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params_w2v['batch_size'],
        epochs=params_w2v['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models_w2v[cell] = model
    histories_w2v[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} (Word2Vec) Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params_w2v['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix

acc_w2v = {}
f1_w2v = {}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nEvaluating {cell.upper()} (Word2Vec)...')
    model = models_w2v[cell]
    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
    y_true_labels = y_test.flatten()
    padding_idx = tag2idx[list(tag2idx.keys())[0]]
    mask = y_true_labels != padding_idx
    y_true_labels = y_true_labels[mask]
    y_pred_labels = y_pred_labels[mask]
    acc = accuracy_score(y_true_labels, y_pred_labels)
    f1 = f1_score(y_true_labels, y_pred_labels, average='macro')
    acc_w2v[cell] = acc
    f1_w2v[cell] = f1
    label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
    target_names = [idx2tag[i] for i in label_indices]
    report = classification_report(
        y_true_labels, y_pred_labels,
        labels=label_indices,
        target_names=target_names,
        zero_division=0
    )
    cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)
    print('Accuracy:', acc)
    print('F1-score:', f1)
    print('Classification Report:\n', report)
    fig, ax = plt.subplots(figsize=(20, 16), dpi=100)
    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names).plot(ax=ax, xticks_rotation=45)
    plt.title(f'Confusion Matrix: {cell.upper()} (Word2Vec)')
    plt.tight_layout()
    plt.show()

import pandas as pd
import seaborn as sns

metrics_w2v = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_w2v['rnn'], acc_w2v['lstm'], acc_w2v['gru'], acc_w2v['bilstm']],
    'F1-score': [f1_w2v['rnn'], f1_w2v['lstm'], f1_w2v['gru'], f1_w2v['bilstm']]
}
metrics_df_w2v = pd.DataFrame(metrics_w2v)
sns.set_theme(style="whitegrid")
fig, ax = plt.subplots(figsize=(12, 8))
metrics_df_w2v.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)
ax.set_title('Model Performance Comparison (Word2Vec) (Used Macro Avg. of F1 scores) [Epocs-50]', fontsize=14, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')
plt.tight_layout()
plt.show()

"""# Let's increase the Hidden Layers

# Now with 2 hidden layers, 10 epocs
"""

from tensorflow.keras.layers import Input, Embedding, SimpleRNN, LSTM, GRU, Bidirectional, Dense, TimeDistributed, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

def build_model_w2v(cell_type='rnn', units=64, lr=0.001, dropout=0.2):
    model = Sequential()
    model.add(Embedding(input_dim=len(word2idx), output_dim=embedding_dim,
                        input_length=max_len,
                        weights=[embedding_matrix],
                        trainable=False))
    if cell_type == 'rnn':
        model.add(SimpleRNN(units, return_sequences=True, dropout=dropout))
        model.add(SimpleRNN(32, return_sequences=True, dropout=dropout))  # Added second hidden layer
    elif cell_type == 'lstm':
        model.add(LSTM(units, return_sequences=True, dropout=dropout))
        model.add(LSTM(32, return_sequences=True, dropout=dropout))  # Added second hidden layer
    elif cell_type == 'gru':
        model.add(GRU(units, return_sequences=True, dropout=dropout))
        model.add(GRU(32, return_sequences=True, dropout=dropout))  # Added second hidden layer
    elif cell_type == 'bilstm':
        model.add(Bidirectional(LSTM(units, return_sequences=True, dropout=dropout)))
        model.add(Bidirectional(LSTM(32, return_sequences=True, dropout=dropout)))  # Added second hidden layer
    model.add(TimeDistributed(Dense(len(tag2idx), activation='softmax')))
    model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

"""# 10 epocs"""

results_w2v = {}
histories_w2v = {}
models_w2v = {}

params_w2v = {
    'batch_size': 32,
    'epochs': 10,
    'units': 64,
    'lr': 0.001,
    'dropout': 0.2
}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nTraining {cell.upper()} with Word2Vec...')

    model = build_model_w2v(cell_type=cell, units=params_w2v['units'], lr=params_w2v['lr'], dropout=params_w2v['dropout'])

    history = model.fit(
        X_train, y_train_cat,
        batch_size=params_w2v['batch_size'],
        epochs=params_w2v['epochs'],
        validation_split=0.2,
        verbose=1
    )

    models_w2v[cell] = model
    histories_w2v[cell] = history

    # Plot Train vs Validation Loss
    plt.figure(figsize=(8, 4))
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Val Loss', marker='x', linestyle='--')
    plt.title(f'{cell.upper()} (Word2Vec) Loss Curve')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.xticks(range(0, params_w2v['epochs']))
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, f1_score, classification_report, confusion_matrix

acc_w2v = {}
f1_w2v = {}

for cell in ['rnn', 'lstm', 'gru', 'bilstm']:
    print(f'\nEvaluating {cell.upper()} (Word2Vec)...')
    model = models_w2v[cell]
    y_pred = model.predict(X_test)
    y_pred_labels = np.argmax(y_pred, axis=-1).flatten()
    y_true_labels = y_test.flatten()
    padding_idx = tag2idx[list(tag2idx.keys())[0]]
    mask = y_true_labels != padding_idx
    y_true_labels = y_true_labels[mask]
    y_pred_labels = y_pred_labels[mask]
    acc = accuracy_score(y_true_labels, y_pred_labels)
    f1 = f1_score(y_true_labels, y_pred_labels, average='macro')
    acc_w2v[cell] = acc
    f1_w2v[cell] = f1
    label_indices = [i for i in sorted(idx2tag.keys()) if i != padding_idx]
    target_names = [idx2tag[i] for i in label_indices]
    report = classification_report(
        y_true_labels, y_pred_labels,
        labels=label_indices,
        target_names=target_names,
        zero_division=0
    )
    cm = confusion_matrix(y_true_labels, y_pred_labels, labels=label_indices)
    print('Accuracy:', acc)
    print('F1-score:', f1)
    print('Classification Report:\n', report)
    fig, ax = plt.subplots(figsize=(20, 16), dpi=100)
    ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names).plot(ax=ax, xticks_rotation=45)
    plt.title(f'Confusion Matrix: {cell.upper()} (Word2Vec)')
    plt.tight_layout()
    plt.show()

import pandas as pd
import seaborn as sns

metrics_w2v = {
    'Model': ['RNN', 'LSTM', 'GRU', 'BiLSTM'],
    'Accuracy': [acc_w2v['rnn'], acc_w2v['lstm'], acc_w2v['gru'], acc_w2v['bilstm']],
    'F1-score': [f1_w2v['rnn'], f1_w2v['lstm'], f1_w2v['gru'], f1_w2v['bilstm']]
}
metrics_df_w2v = pd.DataFrame(metrics_w2v)
sns.set_theme(style="whitegrid")
fig, ax = plt.subplots(figsize=(12, 8))
metrics_df_w2v.plot(
    x='Model',
    y=['Accuracy', 'F1-score'],
    kind='bar',
    ax=ax,
    width=0.7,
    color=['skyblue', 'salmon'],
    edgecolor='black'
)
ax.set_title('Model Performance Comparison (Word2Vec) (Used Macro Avg. of F1 scores) [Epocs-10] [2 hidden layers]', fontsize=14, weight='bold')
ax.set_ylabel('Score', fontsize=14)
ax.set_xlabel('Model', fontsize=14)
ax.set_ylim(0, 1.05)
ax.tick_params(axis='both', labelsize=12)
ax.yaxis.grid(True, linestyle='--', linewidth=0.5)
for p in ax.patches:
    height = p.get_height()
    ax.annotate(f'{height:.3f}',
                (p.get_x() + p.get_width() / 2., height + 0.01),
                ha='center', va='center', fontsize=11, color='black')
plt.tight_layout()
plt.show()